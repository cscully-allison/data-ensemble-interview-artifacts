R1  0:00  
Hello, I start recording here real quick, okay. And then I'm just gonna give you a quick overview of some of the things that are that are in that document. But just to make sure that we're all on the same page about what's going on today. And before I begin with that, I also just want to thank you a lot for participating. I appreciate that your time is very valuable, and so it means a lot for me to give you give me 30 minutes. And then as far as [R2] up in here, this what we're doing is like a one on one interview [R2] in here, but [they will] take notes on the side and may chime in occasionally, but it's mostly. So overall, what we're going to be doing is I'm just going to ask you a series of prompts, you know, regarding how you think about the performance data you work with, probably as a part of the [programmatic performance analysis] project primarily. And at the after that I'm gonna ask you to draw this data for us. And throughout this whole process, I just want to emphasize that there are no right or wrong answers, just 100%, whatever you feel like your opinions, your thoughts, okay. And then also, I'm going to make an audio and video recording of this interview. And I'm also going to keep a copy of the drawings that you produce. And the audio, and the audio will be transcribed and then anonymized. So it shouldn't be able to link back to you at all. And only the research staff, me and [R2] will have access to the original recordings. Okay. And in addition to that, I just wanna let you know that you can choose to leave or in the interview at any time, and this won't affect your relationship with the interview team or anyone associated. All right. Cool. All right, then, let's get started. I'm gonna start off with sort of a basic question. So what project do you work on that uses ensembles of data?

P4  1:49  
I mean, it's kind of interesting, because a lot of the projects we're working with uses some of the data is actually application data, not performance data. But for the performance data, we are basically dealing with different programs, executions, and there's different these programs are instrumented and they typically collect different performance counters, these might be exposed to the CPU, these can be software counters that are kind of made up just for the purpose of a special investigation into some performance question. And that instrumentation in this case is performed with the Caliper tool out of Lawerence Livermore. That basically is a notation based approach that requires to do some changes to the application, but then you can collect different performance counters for dedicated regions, that you define yourself. And typically the applications we are working with, and they will be implemented this way, they do have a few parameters of themselves. And we are working on different architectures and platforms. And this leads to these counters being expressed differently. So we would like if we want to do comparative studies to inform maybe the choice of a library or compiler, we would look at multiple measurements of that. And also multiple measurements across different of these input parameters, for example, to see how the behavior and others is changing, in particular, how the performance might improve or degrade, according to that.

R1  3:17  
Okay. And you mentioned [performance mesaurement tool], is one of the you're working primarily with on the [performance measurement tool] project?

P4  3:25  
Um, well, so my role in the project is, I'm a research assistant professor. And I have been working with a few students that are working a lot with this data. So they are working closely with the team at [National Lab], that is, on the one hand, improving tools to analyze this data. And on the other hand, is also trying to work together with science teams that actually have a need for recommendations, they typically would lack the technical background to have a good understanding. And they also don't have necessarily the capacity to perform the studies themselves. So they're relying on external resources, such as the team at [National Lab], but also research partners and universities, to give them guidance on how to do performance tuning based on different architectures and expertise that we bring in. So I have a strong background in performance engineering, mostly from the IO side. But then a lot of the things, they always overlapping IO and the compute capability says some relationship because if you're producing data of similarly, you have your bottlenecks shifting around as you're tuning different sides of the system. So maybe you tune your compute. An IO becomes a bottleneck and you tune your iOS and compute might become a bottleneck again, and so on. So because of this, I have a relatively strong background in that I also work with other teams at [National Lab 2], for example, they have an instrumentation solution that looks particularly at higher libraries, not so much CPU counters, and we ran into similar analysts problem there because we are dealing with multi dimensional data. And then we have to slice and cut through the data to get like a view that allows for fair comparisons. And like this knowledge I basically use to mentor students here at [University] and also with cross institutional teams as we have in the pay for collaboration, for example.

R1  5:22  
Thank you. So I guess to follow up on this, then could I get you to just describe a, what a typical data ensemble would look like in the space that you work in?

P4  5:36  
So I think the key is that there needs to be some common core, because like, the example would be structured about something that has some commonality, and then you will do perturbations to the input parameters or the execution environment like this, there's really no limit to what you could consider that. I think that is also where, yeah, we, why we are developing these tools in the first place now. I tend to be very flexible and fuzzy in the definition there, because my experience has just said, it doesn't even make too much sense to constrain it so so much. But the downside to remaining fuzzy is that like, you, it's very hard to have a very general tool, I think, if you want to do analysis, it has to be more specialized in many cases. So in that sense, if we are looking at studying performance, for compute optimization, for example, then we will typically try to construct an ensemble that has varying parameters, and would construct the instrumentation so that we are not capturing things that are subject to resource to too much noise or to share resources. So that the, like the measurements we are collecting for the sample can be conclusive to an extent. So I think there's this care the assumptions that you have to do in how you are setting up the measurements for your sample. And then there's the aspect of Yeah, analyzing the ensemble, but there's actually a lot of thought that should go into the measurement process to begin with.

R1  7:16  
Okay. And I guess just to kind of probe the structure of this data, can you tell me what kind of you would see as like the main parts of the data that that would be produced as part of a data ensemble or used as part of the data ensemble?

P4  7:32  
Okay, yeah. So I think what use useful is to kind of, we're nesting multiple contexts into each other. And that isn't the whole thing. So there's different. So like, we have to capture to an extent in the in the data that we have for the [unintelligable], we have to capture the context so that we describe the context to the extent that we can run do the comparisons. So the context includes things like the compiler, libraries, build dependencies, the actual hardware that things are being run on. And this is something that increasingly, we can capture automatically, there's libraries that would kind of formalize and collect these things in a standardized way. And then there's something for the application, the applications have parameters. And this is probably the part that has like the broadest spread, because depending on the application, you are always in, check them into your sample, some custom variables that matter to the application team. And these are completely unpredictable. But you have to be flexible to capture these as well, because they can have a profound impact on what is being measured with instrumentation. And then there's a part of the actual metrics that the instrumentation is gathering. And this is typically constrained by the architectures to an extent. So there's a relationship between the instrumentation and execution environment. But there's also the relationship of these annotated regions. So there's some user defined things. And then there's some parts that are artifacts, because of the systems and the systems and instrumentation was Caliper, for example, that it's under the hood, it's using some other libraries that are well established in the community. So they would be somewhat more translatable to other science teams. So like, I think there's these things that are more general and that apply to multiple use cases. And then there's things that are specific to the use case of a particular application. But it's basically a multi dimensional space of, of parameters to care about. And then there's also the question for the instrumentation data. In many cases, it's not really feasible to collect every event that is trackable, because it gets too expensive or one is interfering with the simulation and all these effects in parallel programs where things become serialized that you don't want to and then like you lose all the benefits of the parallel execution, and then you can reason about what happened in on a parallel performance scenario. For example, like, just as an example, for undesired side effects if one is too invasive in in the instrumentation. But like, in principle, there's a full spectrum of aggregated data that is kind of profiling a region and just preserving some characteristics that are kind of accumulated over time. There's also more fine granular data that might be trace data that captures really individual events.

R1  10:39  
Um, would you say that any of these parts of the data that you just described to me are, like more important than others? Or if there is any sort of hierarchy of importance to them?

P4  10:49  
I don't think that this, like an absolute hierarchy, you could construct, I think it boils down to the question that you're investigating what is important, and that basically informs us the granularity I was mentioning, but also the parameters that you are capturing. So in principle, there's, for example, there's trade offs that if you are capturing fewer metrics, you might be able to capture a longer period of time for for a particular kind of phenomena that you are tracking. So without disturbing or skewing the the application that you are observing. So there's these trade offs in that sense. And if you are analyzing, I think it also depends on the objective, or like, I mean, I think, what's more general way to say that it depends on the question you have is that they might be different objectives. So if you want to optimize the time to solution, there's definitely a high importance to track the time certain things take. If you want to optimize for energy to solution, then you have to look for for different metrics, and even some things that we are not currently capturing. But it really depends on the objectives that you want to take into account. And then obviously, there's often a combination of these, maybe you want result in a reasonable time, but also in a reasonable energy envelope. And then you have to track logs. So I don't think that you can construct a strict hierarchy. It's just that like, time to solution is something that is very often captured. So you might want to have tooling that is very capable to automate questions that are commonly asked for for time based optimizations, for example.

R1  12:37  
Gotcha. Thank you. All right, cool. Well, now we can move on to the interactive portion. I don't know if you have a tablet, or just a pencil and paper. But great, thanks. If I can get you to, maybe if you can point your webcam down at the pencil a piece of paper, I'm going to have you draw. Yeah, I'm gonna have you draw out sort of the entirety of this dataset that you just described, we have this like, typical data set, and I want you to feel free to use any abstractions or metaphors, you want to convey the idea, it doesn't have to be very little.

P4  13:12  
Okay, yeah, this is kind of tricky. It has potential to get very messy. 

R1  13:18  
That's totally fine. 

P4  13:19  
I would start in the execution environment, maybe. And I think typically, you would start in the simplest case, but then in HPC at least, we're often dealing in distributed environments. So I think we have different things that are generating activity based on how the system is being stressed. And we have our application. And if it's a parallel application, there's some coordination going on. So it's connected over a network. And one could kind of imagine that one can have different probes and different spots that happen there. And then we would typically have a buffer of measurement data that we can afford. That's that we can afford. And then like, somehow, this is being distilled into a single profile that we have now for the caliper data, for example, this. . .

R2  14:23  
It might be helpful to turn off the blur. 

P4  14:27  
Oh, yeah. That's a good point. [P4 disables blur on webcam]

R2  14:29  
If you're comfortable with it. Thanks.

P4  14:32  
Yeah, that makes that makes sense. Thanks for pointing that out. I completely forgot I had it on. So like, without reasoning too much about houses, data's being flushed to a final profile. This, I guess it's two things to consider that we are performing some sort of filtering on aggregation on that. And there's different products on different things. Add on the system. So each of these things might be a compute node. Each of the dots might be a process, and that that is running on a CPU, we have multiple processes, but and then they ask the question about how things have been instrumented in the first place. And what we have as a structure in with data from coming from Kelapa is typically what is called graph or call tree. So we are kind of capturing the call stack. So we have we enter a program in the main function. And then there's different there's different sub calls to regions, or functions, that are being instrumented. And we can name these so we have one, maybe we have something that is a little bit more meaningful, talking about calculations, maybe there's something like post processing. And they might have sub functions as well, where the post processing is having their own calculations in the context as well. And for this, at least, for the parallel case, there's two dimensions that kind of unfold. So like this happens on multiple rings, or in multiple processes. So there might be multiple values attached to that. And then for each of the processes, we might be capturing multiple fields of data. And sometimes this data could have a hierarchy of itself. But for most of the data that we are working with, we don't really capture that hierarchy, it's kind of condensed into something that is relatively flat. Also, because it becomes a little bit easier with some of the tooling that we are developing to automate things if there is not additional hierarchy to take care of, because like, each level of hierarchy might be sort of a convention that one has sent to accommodate. So flattening the data, in many cases make things a lot easier to work with. I think that is at least true for the data for the ticket and Ellen and her work. This some other hierarchies and other contexts, where one has data in one observes is through passing through different layers in the software stacks. So I don't know, worrying from the IO perspective. And maybe I'm drawing a little bit of a line here, that should separate this as a new thought. We still have the picture of multiple processes doing things on different things like but we have an application. And on the high level, the application might interact with the library such as HDF, five. So this is a self describing data format has been very commonly used can in scientific applications. But then internally, this might be using IO parameters provided by MPI, for example, which internally might be using POSIX. Now, this can be mapped to the graph perspective as well. But typically, there's some activity where you send a request here, then it unfolds into multiple requests on the lower levels, something like that, is something that happens? So there's a few different structures to data, but I think this is two things that I've commonly seen.

R1  18:36  
Okay, perfect. And when you were talking earlier about you said, the data is hierarchical, but you like collapse it Do you-- Are you referring to the calling context tree specifically? Or is there some other element of this hierarchical that that gets flattened out?

P4  18:55  
So, I mean, I think that the data is hierarchical, is often an artifact, how we are designing systems, and they just tend to be hierarchical, because it's manageable. And also, like easy to address with was relatively simple circuitry. But in principle, so like, I also mentioned that, like this, this perspective on the data is not really capturing what the program is doing the program is, is having more of a graph structure where like in main you you are calling the post processing, for example, and then it's calling the calculate p. And, like, maybe there's a case where this internally might pause the same calculation function that is also used in another step for example. So like things could occur in multiple cases, just because programs can call functions from all sorts of places. So we are using this collapse view because we can still kind of for for a particular call, say like we added a second cog function, we can look at the stack, and then we can aggregate around that, but like, we're doing this mostly because we have to, because we can't afford to, to store like this at the exact granularity that we have in most cases.

R1  20:20  
Thank you. Alright, so then I've just got some analysis based questions for you here kind of things that you might do with a dataset like this. And so if I could get you so I, if I want to, say relate the number of like, MPI threads that I have running, and I want to see how that relates to the actual, like runtime of a single function, what portions of this dataset would I use?

P4  20:48  
Um, so. So I'm not completely sure if I understand the question, but if I think I have an MPI application that has multiple ranks, then each of the ranks is, is going to take some time. So like, the application has to spawn some startup time, which might not even be accounted for in the instrumentation that we have, depending on how we do it. But I think like the most common notion of saying like, okay, the execution time was off, that was from the startup time until the program terminates again. And so it also is true for all the ranks that have to terminate. And typically at the end, when you call the MPI finalize, which every correct MPI application should do, at least to ensure consistency is that you have since joined, like you, you are familiar from threads. So there's a barrier that happens, all the processes or has . . ., they might not be a very good, like, they should be a good signal that like, okay, all the processes, finished their work and have sent it off, to an extent. So I think it depends again, on what you are after, if you look at the whole app program execution, obviously, you have to wait until all your different MPI processes finished. And then you can say, like, okay, the execution ended there. If you have a face in your MPI application that you care a lot about, then you could take a similar model where you can say like, Okay, you were waiting for a bunch of the workers or the different ranks to computers. But in principle, you always have some second off like, okay, it's, it's, it's being finished there, then what we often have an MPI application, just the way it's programmed. And also because it's easier to reason about is. So like, I think it makes sense to look what is kind of distributed across the MPI ranks. And typically, we have some application domains. So this might be a volume or space. And then certain portions of says, this problem domain, how being advanced in a computation. So if we have a simulation, maybe we want to simulate in time, and then we can say like, based on the information that we have in a particular cell, and in the neighborhood, we can compute the next value for the next time step. And then you will do this for a couple of time steps. And then at some point, you reach to a point where maybe you are waiting for data from a neighbor that you have to request from from a neighboring application from a neighboring MPI ranks. Sorry for that. And on the other hand, so like you're working on these chunks of data that you can process in parallel, without being dependent on each other. But typically, there comes this point where you have to communicate again. And then there's also the part where the portion of work that you got assigned as a rank, it might be more or less hard. So like sometimes there's more computations involved with finishing that chunk in comparison to what another rank is having. And this typically leads to some spread about how quickly different MPI ranks are finishing. There's also the effect where there's just manufacturing variability. So some processes happen to be just a little bit faster than others. And then like this can also contribute to different ranks taking different amounts of time to finish more or less a similar problem chunk. So all these low balancing aspects for example, as something that's like huge research field was many good approaches to do that. But it was also many areas where it's not at all sort of, and but like from an analysis perspective, what when all Who cares about his son's distribution of, of how long does do certain ranks need for work, and you would try to make them take all the same time because then like your prioritization becomes easier, you don't waste resources, because you're waiting maybe for only a single rank, that happens to take a little longer. So there's, there's all these kinds of questions that happen. But it again, depends on where you might feel like you have opportunity to reduce cost or where you can. Where you yeah, I think where you have opportunity to reduce costs, but so like having a good idea of a longtail where certain ranks might be running a little long as it is typically something where you get something where you can act on because you know, that maybe this particular rank, you want to assign less work so that your overall work becomes more balanced. And that in the scheme of iterations that you work through, you are always taking more or less the same time to not waste resources, where all the other resources that are idling, they're still going to maybe consume some energy or they are blocking somebody else that could be doing useful work.

R1  26:21  
Then let me see for like my next little task related questions fairly similar. So like, let's say that you want to compare the runtime differences between a code base that was run on one architecture, and also the same code base that was run on a different architecture? Can you show me which of the parts of the data that you drew will be most important for that? If you'd like you can point the camera. Please think think about it. Take your time.

P4  26:52  
So to be honest, actually, I think this figure is not particularly appropriate. But we can take this execution environment they in principle, we have to ensure some level of comparability. So if we are running a system on say, three different nodes, then like the scale that we're comparing should be similar for the most part. Now, there's some interesting questions even in comparing between different configurations. But in certain sense, we always need some basis that we share so that we can do a useful comparison. Now for the question, we want the runtime on two different architectures, we could kind of imagine that we, we have two different architectures, architecture one and architecture two. Then we should make sure that the experiments are comparable for that matter. But it's not always completely possible, because sometimes we don't have the same performance counters. So that is maybe one problem to keep in mind. But considering we have similar performance counters, that we can collect from the systems, then, like, I think the key is that we have the same metrics that we can collect, and then do the runtime analysis. Hey, I don't i i Actually, I wouldn't if I want to keep it general, at least. I wouldn't constrain it much more. I think for a more specific question. I could give a more specific example.

R1  28:35  
Okay, yeah, absolutely. That That makes sense. Then, all right, I've got I've got one last question for you. And this may be specific, but we'll, we'll figure it out. So like, let's say you have a function with very low exclusive time, but high inclusive time? How would you find out where that where all that time is spent? Where that function is?

P4  28:59  
Um, so we had high exclusive time. And low inclusive time.

Let's see. Okay, we saw like, first maybe just to clarify where we are when if you're on the same page for the so like, the exclusive time is where we have. So we are we have the exclusive time here. And we say like this is two seconds where we know we're exclusively spending and that we have one second and one second was inclusive time would be giving us four seconds for this because like that's all the time we are spending right? Yes, exactly. Okay. So

R1  29:53  
did I say it the wrong way? I think I meant to say hi, inclusive time, low XClusive time. Okay, so hi inclusive time. Yeah.

P4  30:03  
Okay, no worries. So, so we are looking for the. So in principle, it depends if we have both the both of these times, then it is kind of already. So like this, there's a few things, right. So like these, depending on the data that we have, and like the way that exclusive and inclusive time is used for for Caliper, for example, is that there may be reasons in this program that are annotated. And so let's make this a little higher, let's have the three seconds because we are spending some time that is unaccounted for. And there's, there's an aspect like this, and then we want to, can you can you just rephrase the question, we clicked just so that, um, so we have,

R1  30:59  
yeah, so like, let's say, at a root at a root node, or at a roof function, it's taking a lot of time in total, right, like all the things that are called under it, and it is taking a lot of time in total, but then the, but it itself, that function itself is not taking a lot of time, a lot. So there's got to be somewhere inside of there, where all the time has got you.

P4  31:22  
Okay, yeah. So like if we have so like, if we only have the inclusive time, and we have that for all the things and we can obviously do some subtraction and kind of narrow that down. If we do take the exclusive time we get a more precise measurement, in principle, because we are taking the times just before and after this region is being accessed. So in, in principle, but like, Yeah, I'm still not completely sure if I may have missed the core of the question, because for me, it's like, I don't know, like, I can kind of trust the instrumentation. And then I can just traverse the tree and kind of pick out the elements that I think are contributing to that.

R1  32:14  
I think that's an appropriate answer to the question, honestly, is that yeah, you have the measurements, you can traverse the tree. That makes a lot of sense to me.

P4  32:24  
All right. Yeah. There's cases, though, where we have seen lopsided would have only captured so like, for Intel counters, for example. They don't happen to capture both these timings. And then we can only narrow down the exclusive times in some cases. So there's some relationship between these obviously. Yeah.

R1  32:53  
Okay. Thanks, [P4]. All right, got one final question for you, then it's gonna be one more drawing, okay, you just say I'm gonna have you put it back down again. And this one, I'm just going to ask you drop a pretty basic data set. It's going to be a profile. So imagine a very generic profile. And it's a single program with three functions. And those functions, we'll just call them A, B, and C. And we'll say that a calls both B and C. And then they all just have time. And so function A took 30 seconds to run. function B took five seconds. And function C took 100 seconds. Cool. Alright. And that's it. And if I could just get you to draw that out for me, and use again, any abstractions or metaphors you like, you don't have to be very little. Okay?

P4  33:55  
So I'm not sure why you can see that but in principle. So like, I think there's a few cases, there's a few cautious I mean, I make some assumptions. If you say like a is calling B and C, then I would imagine a function where you have your, your function a and calling function B, and function C, in when it's drawn like this maybe in in sequence, and then I would get a tree like this. Now, this is not necessarily how an instrumentation might need to captions, this could also be that C might be another might be called indirectly by B or something like that. So I'm not sure if you are capturing these scenarios as well. But for the case where A and the most literal sense is calling B, and C, I think this That's a fair description. And this would be your time.

R1  35:06  
Yeah, that looks like a pretty, pretty comprehensive drawing to me to have that. All right. Cool. And then I guess that's, that's about it. That's all we got for our questions today. Do you have any questions for us before we depart today?

P4  35:22  
Um, so I'm kind of curious. So is it for your PhD work? That you're sorry, oh, well, what's the Where are you taking this?

R1  35:37  
It's like a combination. So it is definitely for my PhD work. But also, ideally, we should be able to use this to help us build like, better interfaces for explaining the very complex data that is like the ensemble data that we work with with [programmatic performance analysis] project Right? Like think it has its own data model. But I mean, it's very helpful for us to understand sort of the spectrum of what people are imagining of their ensemble data and what parts of it they want to look at which parts of it they're naturally abstracting away or removing, and they don't want to be bothered with all. So yeah, so ideally, this will help [National Lab] and [programmatic performance analysis project], but then also this is just an aspect of my PhD research and trying to understand mental models of generals, or complex data.

P4  36:25  
Cool. Yeah. Yeah, I don't really have any concrete questions. Obviously, we're working on at Fleet close together and the fund works. I think we tend to be on a similar page and in general, but no, I enjoyed this. All right. Great. Thank you so so much.

R1  36:43  
Thank you so much. Really helpful. Yeah. And me and [R2] are just gonna stay here for a sec and debrief. So you're free to go whenever you like. Hopefully, we'll get to meet in person sometime. Yeah. All right. All right.

Transcribed by https://otter.ai
